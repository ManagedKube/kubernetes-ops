---
namespaceOverride: monitoring

prometheus:
  prometheusSpec:
    storageSpec:
     volumeClaimTemplate:
       spec:
        #  storageClassName: gluster
         accessModes: ["ReadWriteOnce"]
         resources:
           requests:
             storage: 50Gi
    additionalScrapeConfigs:
    # Istio scrap endpoints
    # Doc: https://istio.io/latest/docs/ops/integrations/prometheus/#option-2-customized-scraping-configurations
    - job_name: 'istiod'
      kubernetes_sd_configs:
      - role: endpoints
        namespaces:
          names:
          - istio-system
      relabel_configs:
      - source_labels: [__meta_kubernetes_service_name, __meta_kubernetes_endpoint_port_name]
        action: keep
        regex: istiod;http-monitoring
    - job_name: 'envoy-stats'
      metrics_path: /stats/prometheus
      kubernetes_sd_configs:
      - role: pod

      relabel_configs:
      - source_labels: [__meta_kubernetes_pod_container_port_name]
        action: keep
        regex: '.*-envoy-prom'
    # End of istio scrape endpoints

grafana:
  adminPassword: prom-operator
  ingress:
    enabled: true
    annotations:
      kubernetes.io/ingress.class: istio
    hosts:
    - grafana.${domain_name}
    tls:
    - hosts:
      - grafana.${domain_name} # This should match a DNS name in the Certificate
      # Istio doc: https://istio.io/latest/docs/ops/integrations/certmanager/#kubernetes-ingress
      # If using istio and using the domain-wildcard cert, the cert-manager kind: certificate
      # should be created in the istio-system namespace.
      secretName: domain-wildcard # This should match the Certificate secretName
  additionalDataSources:
  # - name: loki
  #   access: proxy
  #   basicAuth: false
  #   basicAuthPassword: pass
  #   basicAuthUser: daco
  #   editable: false
  #   jsonData:
  #       tlsSkipVerify: true
  #   orgId: 1
  #   type: loki
  #   url: http://loki-stack:3100
  #   isDefault: false
  #   version: 1
  - name: Tempo
    type: tempo
    access: proxy
    orgId: 1
    url: http://tempo:3100
    basicAuth: false
    isDefault: false
    version: 1
    editable: false
    apiVersion: 1
    uid: tempo
  ## dashboardProviders is required when importing "dashboards"
  dashboardProviders:
    dashboardproviders.yaml:
      apiVersion: 1
      providers:
      - name: 'default'
        orgId: 1
        folder: ''
        type: file
        disableDeletion: false
        editable: true
        options:
          path: /var/lib/grafana/dashboards/default
  dashboards:
    default:
      k8s-cluster-summary:
        gnetId: 8685
        revision: 1
        datasource: Prometheus
      node-exporter-full:
        gnetId: 1860
        revision: 21
        datasource: Prometheus
      prometheus-2-0-overview:
        gnetId: 3662
        revision: 2
        datasource: Prometheus
      stians-disk-graphs:
        gnetId: 9852
        revision: 1
        datasource: Prometheus
      kubernetes-apiserver:
        gnetId: 12006
        revision: 1
        datasource: Prometheus
      ingress-nginx:
        gnetId: 9614
        revision: 1
        datasource: Prometheus
      ingress-nginx2:
        gnetId: 11875
        revision: 1
        datasource: Prometheus
      istio-mesh:
        gnetId: 7639
        revision: 54
        datasource: Prometheus
      istio-performance:
        gnetId: 11829
        revision: 54
        datasource: Prometheus
      istio-service:
        gnetId: 7636
        revision: 54
        datasource: Prometheus
      istio-workload:
        gnetId: 7630
        revision: 54
        datasource: Prometheus
      istio-control-plane:
        gnetId: 7645
        revision: 54
        datasource: Prometheus

## Configuration for alertmanager
## ref: https://prometheus.io/docs/alerting/alertmanager/
##
alertmanager:

  ingress:
    enabled: false

    annotations:
      external-dns.alpha.kubernetes.io/hostname: alertmanager.internal.managedkube.com
      kubernetes.io/ingress.class: nginx-external
      # certmanager.k8s.io/cluster-issuer: prod
      # certmanager.k8s.io/acme-http01-edit-in-place: "true"

    hosts:
      - alertmanager.internal.managedkube.com

    tls:
      - secretName: cert-manager-tls-cert
        hosts:
          - alertmanager.internal.managedkube.com

  alertmanagerSpec:
    alertmanagerConfigSelector:
      matchLabels:
        # The `AlertmanagerConfig` configs must have this label for 
        # this alert manager to include in the config.
        release: kube-prometheus-stack

    ## AlermanagerConfig to be used as top level configuration
    ## doc: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/user-guides/alerting.md#specify-global-alertmanager-config
    ## This "alertmanagerconfig" must exist for the Alert Manager to startup
    alertmanagerConfiguration:
      name: global-alert-config

  # Alertmanager configuration directives
  # ref: https://prometheus.io/docs/alerting/configuration/#configuration-file
  #      https://prometheus.io/webtools/alerting/routing-tree-editor/
  #
  # config:
  #   route:
  #     receiver: 'null'
  #     routes:
  #     ## This alert always "fires" and it is there to give you an alert to check your entire
  #     ## alerting pipeline.  Use this "Watchdog" alert for testing a system.
  #     ## Uncommenting it means that it will send this alert to null and wont alert anything
  #     ## Commenting this out means that it will send this alert through your alerting pipeline.
  #     - match:
  #         alertname: Watchdog
  #       receiver: 'null'
  #       continue: true
      # - match:
      #     alertname: KubeControllerManagerDown
      #   receiver: 'null'
      # - match:
      #     alertname: KubeProxyDown
      #   receiver: 'null'
      # - match:
      #     alertname: KubeSchedulerDown
      #   receiver: 'null'

      # - match_re:
      #     severity: critical|page|alert|none
      #   receiver: slack
      #   continue: true
      # - match:
      #     severity: warning
      #   receiver: slack
      #   continue: true

      # - matchers:
      #   - matchType: =~
      #     name: severity
      #     value: critical|warning|none
      #   receiver: slack

      # - match_re:
      #     severity: critical|page|alert
      #   receiver: pagerduty-critical
      #   continue: true

    # receivers:
    # - name: 'null'

    # - name: 'monitoring/alert-config/slack-ops'
    #   slack_configs:
    #   - api_url: https://hooks.slack.com/services/xxx/xxx/xxx
    #     channel: alerts_ops
    #     send_resolved: true
    #     text: |-
    #       {{ range .Alerts }}
    #           Annotations:
    #           {{ range $key, $value := .Annotations }} - {{ $key }}: {{ $value }}
    #           {{ end }}
    #           Details:
    #           {{ range .Labels.SortedPairs }} - {{ .Name }} = {{ .Value }}
    #           {{ end }}
    #       {{ end }}
    #     title: '{{ if ne .Status "firing" }}[{{ .Status | toUpper }}]{{ end }} {{ .CommonAnnotations.summary }}{{ .CommonAnnotations.message }}'
    #     title_link: https://alertmanager.internal.managedkube.com
    #     username: slack-kube1

    # - name: 'slack-warning'
    #   slack_configs:
    #   - api_url: https://hooks.slack.com/services/xxx/xxx/xxx
    #     channel: kube-alerts
    #     send_resolved: true
    #     text: |-
    #       {{ range .Alerts }}
    #           Annotations:
    #           {{ range $key, $value := .Annotations }} - {{ $key }}: {{ $value }}
    #           {{ end }}
    #           Details:
    #           {{ range .Labels.SortedPairs }} - {{ .Name }} = {{ .Value }}
    #           {{ end }}
    #       {{ end }}
    #     title: '{{ if ne .Status "firing" }}[{{ .Status | toUpper }}]{{ end }} {{ .CommonAnnotations.summary }}{{ .CommonAnnotations.message }}'
    #     title_link: https://alertmanager.internal.managedkube.com
    #     username: slack-warning-dev-us

    # - name: 'pagerduty-critical'
    #   pagerduty_configs:
    #   - service_key: xxxxx

