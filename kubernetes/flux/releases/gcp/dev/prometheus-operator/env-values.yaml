---
prometheus-operator:

  ## Deploy a Prometheus instance
  ##
  prometheus:

    ingress:
      enabled: true

      annotations:
        external-dns.alpha.kubernetes.io/hostname: prometheus.internal.dev.managedkube.com
        kubernetes.io/ingress.class: nginx-internal
        certmanager.k8s.io/cluster-issuer: prod
        certmanager.k8s.io/acme-http01-edit-in-place: "true"

      hosts:
        - prometheus.internal.dev.managedkube.com

      tls:
        - secretName: grafana-general-tls
          hosts:
            - prometheus.internal.dev.managedkube.com

      ## Service type
      ##
      # type: LoadBalancer

    ## Settings affecting prometheusSpec
    ## ref: https://github.com/coreos/prometheus-operator/blob/master/Documentation/api.md#prometheusspec
    ##
    prometheusSpec:

      ## External URL at which Prometheus will be reachable.
      ##
      externalUrl: "https://prometheus.internal.dev.managedkube.com"

      ## Resource limits & requests
      ##
      resources:
        requests:
          memory: 256Mi

      ## Prometheus StorageSpec for persistent data
      ## ref: https://github.com/coreos/prometheus-operator/blob/master/Documentation/user-guides/storage.md
      ##
      storageSpec:
       volumeClaimTemplate:
         spec:
           # storageClassName: gluster
           accessModes: ["ReadWriteOnce"]
           resources:
             requests:
               storage: 20Gi
        #  selector: {}

      ## The remote_write spec configuration for Prometheus.
      ## ref: https://github.com/coreos/prometheus-operator/blob/master/Documentation/api.md#remotewritespec
      # remoteWrite:
      #   - url: https://prometheus-us-central1.grafana.net/api/prom/push
          # basic_auth:
          #   username: 6482
          #   password: xxxx

  ## Configuration for alertmanager
  ## ref: https://prometheus.io/docs/alerting/alertmanager/
  ##
  alertmanager:

    ingress:
      enabled: true

      annotations:
        external-dns.alpha.kubernetes.io/hostname: alertmanager.internal.dev.managedkube.com
        kubernetes.io/ingress.class: nginx-internal
        certmanager.k8s.io/cluster-issuer: prod
        certmanager.k8s.io/acme-http01-edit-in-place: "true"

      hosts:
        - alertmanager.internal.dev.managedkube.com

      tls:
        - secretName: grafana-general-tls
          hosts:
            - alertmanager.internal.dev.managedkube.com

    ## Alertmanager configuration directives
    ## ref: https://prometheus.io/docs/alerting/configuration/#configuration-file
    ##      https://prometheus.io/webtools/alerting/routing-tree-editor/
    ##
    config:
      route:
        receiver: 'slack-tests'
        routes:
        - match:
            alertname: DeadMansSwitch
          receiver: 'null'
        # These are causing a lot of false positives right now.  Turning off for now.
        - match_re:
            alertname: CPUThrottlingHigh
          receiver: 'null'
        # Silencing these alarms because it is from the test namespace
        # - match_re:
        #     # alertname: CPUThrottlingHigh
        #     # container_name: "prometheus-config-reloader|rules-configmap-reloader"
        #     namespace: ibm-system|dev-k8sbot-test-pods
        #   receiver: 'null'
        # # Couldnt find the config to make this configurable, so just turning it off for now
        # # https://github.com/coreos/prometheus-operator/issues/2171#issuecomment-465525316
        # - match_re:
        #     alertname: CPUThrottlingHigh
        #     container_name: "prometheus-config-reloader|config-reloader|rules-configmap-reloader|metrics-server-nanny|metrics-server|heapster-nanny|heapster|default-http-backend"
        #   receiver: 'null'
        # # GKE does not have the kube-* pods running in the kube-system namespace.  Silencing them.
        # - match_re:
        #     # alertname: CPUThrottlingHigh
        #     # container_name: "prometheus-config-reloader|rules-configmap-reloader"
        #     alertname: KubeControllerManagerDown|KubeSchedulerDown
        #   receiver: 'null'
        # # Silencing CPU/memory over commit since it is a little chatty and we are over commiting
        # - match_re:
        #     alertname: KubeCPUOvercommit|KubeMemOvercommit
        #   receiver: 'null'
        - match_re:
            severity: critical|page|alert
          receiver: slack-critical
          continue: true
        - match:
            severity: warning
          receiver: slack-warning
          continue: true
        - match_re:
            severity: critical|page|alert
          receiver: pagerduty-critical
          continue: true

      receivers:
      - name: 'null'

      - name: 'slack-tests'
        slack_configs:
        - api_url: https://hooks.slack.com/services/xxx/xxx/xxx
          channel: kube-alerts
          send_resolved: true
          text: |-
            {{ range .Alerts }}
                Annotations:
                {{ range $key, $value := .Annotations }} - {{ $key }}: {{ $value }}
                {{ end }}
                Details:
                {{ range .Labels.SortedPairs }} - {{ .Name }} = {{ .Value }}
                {{ end }}
            {{ end }}
          title: '{{ if ne .Status "firing" }}[{{ .Status | toUpper }}]{{ end }} {{ .CommonAnnotations.summary }}{{ .CommonAnnotations.message }}'
          title_link: https://alertmanager.internal.dev.managedkube.com
          username: slack-test-dev-us

      - name: slack-critical
        slack_configs:
        - api_url: https://hooks.slack.com/services/xxx/xxx/xxx
          channel: kube-alerts
          send_resolved: true
          text: |-
            {{ range .Alerts }}
                Annotations:
                {{ range $key, $value := .Annotations }} - {{ $key }}: {{ $value }}
                {{ end }}
                Details:
                {{ range .Labels.SortedPairs }} - {{ .Name }} = {{ .Value }}
                {{ end }}
            {{ end }}
          title: '{{ if ne .Status "firing" }}[{{ .Status | toUpper }}]{{ end }} {{ .CommonAnnotations.summary }}{{ .CommonAnnotations.message }}'
          title_link: https://alertmanager.internal.dev.managedkube.com
          username: slack-critical-dev-us

      - name: 'slack-warning'
        slack_configs:
        - api_url: https://hooks.slack.com/services/xxx/xxx/xxx
          channel: kube-alerts
          send_resolved: true
          text: |-
            {{ range .Alerts }}
                Annotations:
                {{ range $key, $value := .Annotations }} - {{ $key }}: {{ $value }}
                {{ end }}
                Details:
                {{ range .Labels.SortedPairs }} - {{ .Name }} = {{ .Value }}
                {{ end }}
            {{ end }}
          title: '{{ if ne .Status "firing" }}[{{ .Status | toUpper }}]{{ end }} {{ .CommonAnnotations.summary }}{{ .CommonAnnotations.message }}'
          title_link: https://alertmanager.internal.dev.managedkube.com
          username: slack-warning-dev-us

      - name: 'pagerduty-critical'
        pagerduty_configs:
        - service_key: xxxxx

    ## Configuration for Alertmanager service
    ##
    # service:
    #   annotations:
    #     service.beta.kubernetes.io/aws-load-balancer-ssl-cert: arn:aws:acm:us-east-1:xxxx:certificate/7385702b-7ef5-4233-b5c4-ab48a88eef58
    #     service.beta.kubernetes.io/aws-load-balancer-connection-idle-timeout: "3600"
    #     service.beta.kubernetes.io/aws-load-balancer-internal: "0.0.0.0/0"
    #     external-dns.alpha.kubernetes.io/hostname: "alertmanager.example.com"

      ## Service type
      ##
      # type: LoadBalancer

    alertmanagerSpec:
    #   externalUrl: https://example.com:9093
      logLevel: debug

  grafana:
    ingress:
      enabled: true

      annotations:
        external-dns.alpha.kubernetes.io/hostname: grafana.internal.dev.managedkube.com
        kubernetes.io/ingress.class: nginx-internal
        certmanager.k8s.io/cluster-issuer: prod
        certmanager.k8s.io/acme-http01-edit-in-place: "true"

      hosts:
      - grafana.internal.dev.managedkube.com

      tls:
      - secretName: grafana-general-tls
        hosts:
        - grafana.internal.dev.managedkube.com

    ## Configure additional grafana datasources
    ## ref: http://docs.grafana.org/administration/provisioning/#datasources
    # additionalDataSources:
    # - name: prometheus-sample
    #   access: proxy
    #   basicAuth: true
    #   basicAuthPassword: pass
    #   basicAuthUser: daco
    #   editable: false
    #   jsonData:
    #       tlsSkipVerify: true
    #   orgId: 1
    #   type: prometheus
    #   url: https://prometheus.svc:9090
    #   version: 1
